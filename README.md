# SSPO (Semi-Supervised Preference Optimization)

This repository contains the implementation of SSPO and related methods (DPO, ORPO, SimPO, SSRM).

## Installation

1. Create a virtual environment named 'sspo' with Python 3.10 or higher:
```bash
python -m venv sspo
source sspo/bin/activate  # For Linux/Mac
# or
.\sspo\Scripts\activate  # For Windows
```

2. Install required packages:
```bash
cd SSPO
pip install -r requirements.txt
```

## Execution

### SSPO Training

1. Preprocess the data:
```bash
python preprocessing_data/preprocessing_ultrachat.py --fb [feedback_ratio] --ch [chat_ratio]
```

2. Generate YAML configuration and training command:
```bash
python examples/train/make_yaml.py
```

3. Execute training:
```bash
# Copy the generated command from make_yaml.py output
# Paste it into examples/train/train.sh
bash examples/train/train.sh
```

### DPO, ORPO, SimPO Training

Follow the same steps as SSPO, but modify the source path in the respective shell script:
- For DPO: Modify `examples/train/train.sh`
- For ORPO: Modify `examples/train/train.sh`
- For SimPO: Modify `examples/train/train.sh`

### SSRM Training

1. Generate additional unlabeled responses:
```bash
python examples/SSRM/generate_responses.py
```

2. Perform pseudo-labeling using a pre-trained reward model:
```bash
python examples/SSRM/pseudo_label.py
```

3. Filter data based on confidence threshold:
```bash
python examples/SSRM/conf_threshold.py
```

4. Merge feedback data:
```bash
python examples/SSRM/merge_json.py
```

5. Execute the complete SSRM training pipeline:
```bash
# Configure the number of iterations in examples/SSRM/train-ssrm.sh
# The script will execute steps 1-4 for the specified number of iterations
bash examples/SSRM/train-ssrm.sh
```

## Notes

- Make sure to adjust hyperparameters in the YAML configuration file generated by `make_yaml.py`
- For SSRM, you can control the number of iterations by modifying the commands in `train-ssrm.sh`