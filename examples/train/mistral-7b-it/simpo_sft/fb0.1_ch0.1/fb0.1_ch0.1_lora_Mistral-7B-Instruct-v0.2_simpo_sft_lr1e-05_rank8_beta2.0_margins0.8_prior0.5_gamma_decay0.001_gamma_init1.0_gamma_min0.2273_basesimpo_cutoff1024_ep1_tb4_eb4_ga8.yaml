cache_dir: ./cache/mistral-7b-it
cutoff_len: 1024
dataset: ultra_combined_fb0.1_ch0.1
ddp_timeout: 180000000
do_train: true
eval_steps: 300
eval_strategy: steps
finetuning_type: lora
gradient_accumulation_steps: 8
learning_rate: 1.0e-05
logging_steps: 20
lora_rank: 8
lora_target: all
lr_scheduler_type: cosine
max_grad_norm: 1.0
max_samples: 10000000
model_name_or_path: mistralai/Mistral-7B-Instruct-v0.2
num_train_epochs: 1
output_dir: ./saves_Mistral-7B-Instruct-v0.2/fb0.1_ch0.1/lora_Mistral-7B-Instruct-v0.2_simpo_sft_lr1e-05_rank8_beta2.0_margins0.8_prior0.5_gamma_decay0.001_gamma_init1.0_gamma_min0.2273_cutoff1024_ep1_tb4_eb4_ga8
overwrite_cache: true
overwrite_output_dir: true
per_device_eval_batch_size: 4
per_device_train_batch_size: 4
plot_loss: true
pref_beta: 2.0
pref_loss: simpo_sft
preprocessing_num_workers: 12
save_steps: 300
simpo_gamma: 0.8
stage: dpo
template: default
trust_remote_code: true
val_size: 0.1
warmup_ratio: 0.1
